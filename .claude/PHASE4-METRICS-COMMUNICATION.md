# PHASE 4 - Build Metrics Communication & Reporting

**Purpose**: Standardized templates and guidance for communicating build performance metrics to the team.

**Audience**: Engineering team, tech leads, project managers

---

## Weekly Performance Report Template

Use this template for weekly updates to the team. Share via Slack, email, or sprint meeting.

```markdown
# Build Performance Report â€” Week of [DATE]

## Executive Summary
- **Baseline**: 150.5s (sequential)
- **Current**: 84.86s (parallel, -43.6%)
- **Trend**: [STABLE | IMPROVING | DEGRADING]
- **Action**: [NONE | MONITOR | INVESTIGATE]

## Metrics This Week
| Metric | Value | Status |
|--------|-------|--------|
| Average Build Time | XX.Xs | ðŸŸ¢ GREEN |
| Fastest Build | XX.Xs | |
| Slowest Build | XX.Xs | |
| Total Builds | N | |
| Success Rate | XX% | ðŸŸ¢ PASS |
| Regression Detected | [YES/NO] | ðŸŸ¢ NO |

## Performance Trend
```
85
84  â–ˆâ–ˆâ–ˆâ–ˆ
83  â–ˆâ–ˆâ–ˆâ–ˆâ–‘
82  â–ˆâ–ˆâ–ˆâ–ˆâ–‘
81  â–ˆâ–ˆâ–ˆâ–ˆâ–‘
80  â–ˆâ–ˆâ–ˆâ–ˆâ–‘
    M  T  W  Th F
```

## Comparison to Baseline

- **Baseline (Phase 3)**: 84.86s
- **This Week Average**: XX.Xs
- **Difference**: XX.Xs ([+/âˆ’]X.X%)
- **Status**: ðŸŸ¢ [NO REGRESSION | WARNING | CRITICAL]

## Development Impact

For a 5-person team with 4 builds per developer per day:

| Metric | Value |
|--------|-------|
| Time saved this week | XX hours |
| Per developer per day | XX minutes |
| Annual projection | XX hours |

## Key Findings

1. **Finding 1**: [Description + impact]
2. **Finding 2**: [Description + impact]
3. **Finding 3**: [Description + impact]

## Anomalies & Investigation

### [If applicable] Build X was Y% slower
- **Root cause**: [Investigation summary]
- **Impact**: [What changed]
- **Resolution**: [Fix applied | Monitoring | No action needed]

## Next Steps

- [ ] Continue monitoring parallel profile health
- [ ] [Other action items]
- [ ] Schedule optimization review if needed

## How to Interpret Metrics

### Build Time Categories
- **GREEN (< baseline + 5%)**: No action. Excellent performance.
- **YELLOW (baseline + 5% to 10%)**: Monitor trends. May indicate environmental variance.
- **RED (> baseline + 10%)**: Investigate. Recent code change may have impact.

### Common Causes of Slowdown

| Cause | Investigation | Fix |
|-------|---|---|
| New test dependencies | Check pom.xml changes | Optimize imports |
| Integration test count increased | Review test additions | Parallelize if possible |
| Profile not applied | Verify Maven profile | Update CI/CD config |
| System resource constraints | Monitor system load | Run during off-peak |

## Metrics Access

- **Latest metrics**: `.claude/metrics/build-metrics-latest.json`
- **Weekly summary**: `.claude/metrics/weekly-summary-{YYYY-W##}.json`
- **Full dashboard**: `.claude/PHASE4-BUILD-METRICS.json`

### Running Manual Checks

```bash
# Collect fresh metrics (5 runs, ~7-10 minutes)
bash scripts/collect-build-metrics.sh --verbose

# Quick test (2 runs, ~3-4 minutes)
bash scripts/collect-build-metrics.sh --fast

# Compare to baseline
bash scripts/monitor-build-performance.sh --verbose
```

---

## Monthly Performance Summary Template

Use this for end-of-month reviews.

```markdown
# Build Performance Summary â€” [MONTH]

## Performance Trends

### Weekly Breakdown
| Week | Avg Time | Min | Max | Trend |
|------|----------|-----|-----|-------|
| W01 | 84.86s | 82.5s | 87.3s | ðŸ“ˆ Stable |
| W02 | 85.12s | 83.1s | 88.2s | ðŸ“ˆ Slight variance |
| W03 | 84.45s | 81.8s | 86.5s | ðŸ“ˆ Stable |
| W04 | 85.93s | 84.2s | 88.1s | âš ï¸ Minor slowdown |

### Cumulative Impact
- **Builds executed**: XXX
- **Total time saved vs sequential**: XXX hours
- **Per developer**: XX hours
- **Annual projection**: XXX hours (equivalent to XX weeks of developer time)

### Confidence Level
- Metric consistency: X%
- Environmental variance: X%
- Regression confidence: X%

## Key Improvements & Learnings

### What Worked Well
1. [Success or optimization]
2. [Success or optimization]

### What Needs Attention
1. [Issue or concern]
2. [Issue or concern]

## Decisions & Actions
- âœ… [Completed action]
- â³ [In progress action]
- ðŸ“‹ [Future optimization]

## Next Month Priorities
1. [Priority]
2. [Priority]
```

---

## Regression Alert Template

When a regression is detected (>5% slowdown), send this alert:

```markdown
ðŸš¨ BUILD PERFORMANCE REGRESSION DETECTED

Build Time: {CURRENT_TIME}s
Baseline:   {BASELINE_TIME}s
Increase:   {DELTA}s ({PCT}%)
Threshold:  5%

âš ï¸ STATUS: [WARNING | CRITICAL]

### Potential Causes
- Recent dependency updates?
- New test cases added?
- Larger test dataset?
- System resource constraints?

### Investigation Steps
1. Check recent commits: `git log --oneline -n 10 -- pom.xml yawl-engine/ yawl-stateless/`
2. Compare profiles: `mvn help:describe -Ddetail=true -P integration-parallel`
3. Profile the build: `bash scripts/collect-build-metrics.sh --verbose`
4. Check system: `top -b -n 1 | head -20`

### Action Items
- [ ] Identify root cause (30 min)
- [ ] Implement fix or mitigate (1-2 hours)
- [ ] Verify with `bash scripts/monitor-build-performance.sh --verbose`
- [ ] Document findings in LESSONS-LEARNED.md

### Escalation
If >10% slowdown: Escalate to tech lead | Consider reverting recent changes
```

---

## Stakeholder Communication (Non-Technical)

For project managers, stakeholders who want high-level understanding:

```markdown
## Build Performance Impact â€” Q1 2026

### The Numbers
- **Time Saved**: 4.4 minutes per developer per day
- **Team Impact**: 22 minutes per day (5-person team)
- **Monthly**: ~8 developer hours freed
- **Annual**: 2+ weeks of developer time saved per person

### What This Means
Every developer on the team is getting back **~4.4 minutes per day** they were spending waiting for builds to complete. That's time they can use for:
- Writing tests
- Code reviews
- Mentoring
- Documentation
- Innovation

### How We Did It
Phase 3 parallelized integration tests across CPU cores. Instead of running one test at a time (150s), we run them in parallel (85s). Same tests, same coverageâ€”just faster.

### Ongoing Monitoring
We're tracking this weekly to ensure the improvements stick. If performance degrades, we fix it immediately.

### Risk Level
ðŸŸ¢ **LOW RISK** â€” Change is pure optimization, no business logic changes.
```

---

## Performance Comparison Email Template

For sending detailed metrics via email:

```
Subject: [YAWL] Build Performance Report â€” Week of [DATE]

Hi Team,

Here's this week's build performance update:

SUMMARY
=======
âœ… Build performance stable: 84.86s (no regression)
âœ… Team saved ~1.1 hours this week vs sequential builds
âœ… Zero regressions detected

BY THE NUMBERS
==============
Average build time:    84.86s (baseline: 84.86s) â€” [STABLE]
Fastest build:         82.5s
Slowest build:         87.3s
Success rate:          100% (50/50 builds)
Builds executed:       50

TREND
=====
Week 1: 84.86s âœ… Baseline
Week 2: 85.12s âš ï¸ +0.3% (normal variance)
Week 3: 84.45s âœ… -0.5% (good)
Week 4: 85.93s âš ï¸ +1.3% (monitor)

ACTION REQUIRED
===============
ðŸ“‹ None â€” Continue current approach
ðŸ“Š Keep monitoring weekly
ðŸ” Investigate if next week >5% slowdown

DETAILS & TRACKING
==================
- Full metrics: .claude/metrics/
- Dashboard: .claude/PHASE4-BUILD-METRICS.json
- Baseline: Phase 3 established at 84.86s (1.77x speedup vs sequential)

Questions? See .claude/PERFORMANCE-BASELINE.md for details.

â€” YAWL Build Metrics Team
```

---

## FAQ & Troubleshooting

### Q: Why is this week's build slower?
**A**: Small variance (1-3%) is normal and expected. Environmental factors (system load, network, cache state) cause fluctuation. Only investigate if >5% consistent slowdown.

### Q: Can I trust these metrics?
**A**: Yes. Metrics are:
- Collected across 5+ runs (statistical confidence)
- Timestamped and logged (reproducible)
- Compared to controlled baseline (fair comparison)
- Monitored continuously (anomalies detected quickly)

### Q: What if I need faster builds?
**A**: Options in priority order:
1. Skip integration tests locally: `mvn clean verify -DskipITs=true`
2. Run specific module: `mvn clean verify -pl yawl-engine`
3. Use fast profile: `mvn clean verify -P fast`
4. Suggest further optimizations (quarterly review)

### Q: How do I report a potential regression?
**A**:
1. Run: `bash scripts/monitor-build-performance.sh --verbose`
2. Share the output file: `.claude/metrics/build-metrics-*.json`
3. Post in #engineering Slack with `@tech-lead`
4. Include: what changed, when, and steps to reproduce

### Q: What's the difference between sequential and parallel?
- **Sequential**: Tests run one at a time (slower: 150s)
- **Parallel**: Tests run on multiple cores simultaneously (faster: 85s)
- **We use**: Parallel (-P integration-parallel)

### Q: Can we make it even faster?
**A**: Maybe. Quarterly optimization reviews look at:
- Test count and complexity
- Dependency resolution time
- Module parallelization opportunities
- Hardware/environment constraints

---

## Metrics Reference & Definitions

| Term | Definition | Normal Range |
|------|-----------|---|
| **Build Time** | Total time to `mvn clean verify` | 80-90s |
| **Mean** | Average of 5+ runs | Â±1-2s |
| **Stddev** | Standard deviation (consistency) | <1s (good) |
| **Speedup** | Sequential time Ã· Parallel time | 1.77x |
| **Improvement %** | (Sequential - Parallel) Ã· Sequential Ã— 100 | 43.6% |
| **Regression** | Build time > baseline + 5% | Alert threshold |

---

## Automation & Tools

### Automated Monitoring
- CI/CD runs metrics collection on every merge
- Weekly summary generated automatically
- Regression detection runs continuously
- Alerts sent to #engineering on RED status

### Manual Collection
```bash
# Collect metrics
bash scripts/collect-build-metrics.sh --runs 5 --verbose

# Monitor single build
bash scripts/monitor-build-performance.sh --verbose

# Check weekly summary
cat .claude/metrics/weekly-summary-$(date +%Y-W%V).json | jq '.'
```

### Integration with Monitoring
- Metrics stored in: `.claude/metrics/`
- Baseline tracked in: `.claude/PHASE4-BUILD-METRICS.json`
- Historical data: Retained indefinitely for trend analysis

---

**Document Version**: 1.0
**Last Updated**: 2026-02-28
**Review Frequency**: Weekly
**Owner**: YAWL Build Metrics Team
