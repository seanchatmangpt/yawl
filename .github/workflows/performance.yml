name: Performance Regression Detection

on:
  pull_request:
    branches: [master, main]
    paths:
      - 'yawl-engine/**'
      - 'yawl-stateless/**'
      - 'yawl-benchmark/**'
      - 'test/org/yawlfoundation/yawl/performance/**'
  schedule:
    - cron: '0 3 * * 0'  # Weekly on Sunday 3 AM UTC
  workflow_dispatch:
    inputs:
      update_baseline:
        description: 'Update baselines after run'
        required: false
        default: 'false'
        type: boolean

env:
  MAVEN_OPTS: -Xmx4g -Xms1g -XX:+UseZGC -Djava.awt.headless=true
  JAVA_VERSION: 25

jobs:
  benchmark:
    name: Run JMH Benchmarks
    runs-on: ubuntu-latest
    timeout-minutes: 60

    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Set up JDK 25
        uses: actions/setup-java@v4
        with:
          java-version: ${{ env.JAVA_VERSION }}
          distribution: 'temurin'
          cache: 'maven'

      - name: Cache Maven Repository
        uses: actions/cache@v4
        with:
          path: |
            ~/.m2/repository
            ~/.m2/wrapper
          key: ${{ runner.os }}-maven-java25-${{ hashFiles('**/pom.xml') }}

      - name: Check for Baseline
        id: baseline
        run: |
          if [ -f "benchmarks/baseline.json" ]; then
            echo "baseline_exists=true" >> $GITHUB_OUTPUT
            echo "Baseline found with $(jq '.benchmarks | length' benchmarks/baseline.json) benchmarks"
          else
            echo "baseline_exists=false" >> $GITHUB_OUTPUT
            echo "::warning::No baseline found. First run will create baseline."
          fi

      - name: Build Project
        run: mvn clean package -DskipTests -Pfast -T 2C --batch-mode

      - name: Run JMH Benchmarks (Fast)
        run: |
          if [ -f "yawl-benchmark/target/benchmarks.jar" ]; then
            java -jar yawl-benchmark/target/benchmarks.jar \
              -rf json \
              -rff target/jmh-results.json \
              -wi 2 -i 3 -f 1
          else
            echo "No benchmark JAR found, running via Maven"
            mvn test -pl yawl-benchmark -Dbenchmark.mode=fast
          fi

      - name: Detect Regressions
        id: detect
        if: steps.baseline.outputs.baseline_exists == 'true'
        run: |
          echo "Comparing against baseline..."
          # Simple comparison script
          python3 << 'EOF'
          import json
          import sys

          with open('benchmarks/baseline.json') as f:
              baseline = json.load(f)

          with open('target/jmh-results.json') as f:
              current = json.load(f)

          regressions = []
          for benchmark in baseline.get('benchmarks', {}):
              baseline_val = baseline['benchmarks'][benchmark]['baseline']
              threshold = baseline.get('thresholds', {}).get('default', {}).get('regression', 0.20)

              # Find in current results
              for result in current:
                  if benchmark in result.get('benchmark', ''):
                      current_val = result.get('primaryMetric', {}).get('score', baseline_val)
                      change = (current_val - baseline_val) / baseline_val
                      if change > threshold:
                          regressions.append({
                              'benchmark': benchmark,
                              'change': f"{change*100:.1f}%",
                              'threshold': f"{threshold*100:.0f}%"
                          })

          if regressions:
              print(f"::error::{len(regressions)} performance regression(s) detected!")
              for r in regressions:
                  print(f"::error::{r['benchmark']} degraded by {r['change']} (threshold: {r['threshold']})")
              sys.exit(1)
          else:
              print("No regressions detected.")
          EOF

      - name: Upload Benchmark Results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: benchmark-results
          path: |
            target/jmh-results.json
          retention-days: 30

      - name: Generate Summary
        if: always()
        run: |
          echo "## Performance Regression Report" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "| Status | Details |" >> $GITHUB_STEP_SUMMARY
          echo "|--------|---------|" >> $GITHUB_STEP_SUMMARY
          if [ -f "target/jmh-results.json" ]; then
            echo "| Benchmarks Run | Completed |" >> $GITHUB_STEP_SUMMARY
          else
            echo "| Benchmarks | No results |" >> $GITHUB_STEP_SUMMARY
          fi
          if [ "${{ steps.baseline.outputs.baseline_exists }}" == "true" ]; then
            echo "| Baseline | Found |" >> $GITHUB_STEP_SUMMARY
          else
            echo "| Baseline | Not found (first run) |" >> $GITHUB_STEP_SUMMARY
          fi