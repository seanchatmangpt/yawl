name: Build + Remote Cache Sync

# Trigger on: push to main/develop, pull requests
on:
  push:
    branches:
      - main
      - develop
      - 'release/**'
  pull_request:
    branches:
      - main
      - develop

# Environment variables
env:
  JAVA_VERSION: '25'
  MAVEN_VERSION: '3.9.4'
  YAWL_REMOTE_CACHE: '1'
  CACHE_METRICS_ENABLED: '1'

jobs:
  build-with-cache:
    name: Build + Cache Sync
    runs-on: ubuntu-latest

    # Permissions for OIDC + S3 access
    permissions:
      id-token: write  # OIDC for AWS STS assume-role
      contents: read
      checks: write
      pull-requests: write

    steps:
      # Step 1: Checkout code
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0  # Full history for semantic hashing

      # Step 2: Configure AWS credentials via OIDC
      #         (no access keys stored in secrets)
      - name: Configure AWS Credentials (OIDC)
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ secrets.AWS_ROLE_ARN }}
          role-session-name: yawl-ci-cache
          aws-region: us-east-1
          duration-seconds: 3600

      # Step 3: Set up Java
      - name: Set up Java 25
        uses: actions/setup-java@v4
        with:
          java-version: ${{ env.JAVA_VERSION }}
          distribution: 'temurin'
          cache: 'maven'
          cache-dependency-path: '**/pom.xml'

      # Step 4: Configure S3 cache endpoint
      - name: Configure S3 Bucket Name
        run: |
          echo "S3_BUCKET_NAME=${{ secrets.S3_BUCKET_NAME }}" >> $GITHUB_ENV
          echo "S3_REGION=us-east-1" >> $GITHUB_ENV
          echo "S3_ENDPOINT=s3.amazonaws.com" >> $GITHUB_ENV

      # Step 5: Display build environment
      - name: Display Build Environment
        run: |
          echo "Java version:"
          java -version
          echo "Maven version:"
          mvn -version
          echo "Git commit:"
          git log -1 --oneline

      # Step 6: Verify S3 connectivity
      - name: Verify S3 Cache Connectivity
        run: |
          echo "Checking S3 connectivity..."
          aws s3 ls s3://${{ env.S3_BUCKET_NAME }}/ --region ${{ env.S3_REGION }} || {
            echo "âš  WARNING: S3 unreachable, will use local cache only"
            echo "YAWL_REMOTE_CACHE=0" >> $GITHUB_ENV
          }

      # Step 7: Build with remote cache
      - name: Build with Remote Cache
        env:
          YAWL_REMOTE_CACHE: '1'
          YAWL_S3_BUCKET: ${{ secrets.S3_BUCKET_NAME }}
          YAWL_S3_REGION: us-east-1
        run: |
          echo "Building with remote cache enabled..."
          bash scripts/dx.sh --remote-cache all
        continue-on-error: false

      # Step 8: Sync cache metrics to S3
      - name: Sync Metrics to S3
        if: always()
        env:
          YAWL_S3_BUCKET: ${{ secrets.S3_BUCKET_NAME }}
          YAWL_S3_REGION: us-east-1
        run: |
          echo "Uploading cache metrics..."
          bash scripts/remote-cache-management.sh metrics --upload || {
            echo "âš  WARNING: Failed to upload metrics (continuing)"
          }
        continue-on-error: true

      # Step 9: Report cache statistics
      - name: Report Cache Statistics
        if: always()
        run: |
          echo "=== Cache Statistics ==="
          bash scripts/remote-cache-management.sh stats || echo "No stats available yet"
          echo ""
          echo "=== Cache Health Check ==="
          bash scripts/remote-cache-management.sh health || echo "Health check skipped"
        continue-on-error: true

      # Step 10: Upload test results (if tests run)
      - name: Upload Test Results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: test-results-${{ matrix.java }}
          path: '**/target/surefire-reports/'
          retention-days: 5

      # Step 11: Comment on PR with cache stats (if PR)
      - name: Comment PR with Cache Stats
        if: github.event_name == 'pull_request' && always()
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            const statsFile = '.yawl/metrics/remote-cache-stats.json';

            if (fs.existsSync(statsFile)) {
              const stats = JSON.parse(fs.readFileSync(statsFile, 'utf-8'));
              const comment = `
            ## ðŸ“Š Cache Performance

            - **Hit Rate**: ${stats.hit_rate_total_pct?.toFixed(1)}%
            - **Remote Hits**: ${stats.hit_rate_remote_pct?.toFixed(1)}%
            - **Download Time**: ${stats.download_time_avg_sec?.toFixed(1)}s
            - **Cold Start**: ${stats.cold_start_time_avg_sec?.toFixed(1)}s

            Cache is working! âœ“
              `;
              github.rest.issues.createComment({
                issue_number: context.issue.number,
                owner: context.repo.owner,
                repo: context.repo.repo,
                body: comment
              });
            }

      # Step 12: Publish cache metrics to summary
      - name: Publish Metrics to Summary
        if: always()
        run: |
          {
            echo "## Remote Cache Metrics"
            echo ""
            echo "| Metric | Value |"
            echo "|--------|-------|"
            bash scripts/remote-cache-management.sh stats 2>/dev/null | \
              jq -r '.cache_hit_rate_total_pct // .hit_rate_total_pct // "N/A"' | \
              xargs -I {} echo "| Total Hit Rate | {}% |" || echo "| Hit Rate | Error |"
            echo ""
          } >> $GITHUB_STEP_SUMMARY

      # Step 13: Alert on build failure
      - name: Notify on Build Failure
        if: failure()
        run: |
          echo "âš  BUILD FAILED"
          echo "Cache status may not be synced"
          echo ""
          echo "Troubleshoot:"
          echo "1. Check S3 bucket access"
          echo "2. Verify AWS credentials rotation"
          echo "3. Review build logs above"

  # Secondary job: Cache analysis (optional, non-blocking)
  cache-analysis:
    name: Cache Analysis & Reporting
    runs-on: ubuntu-latest
    if: always()
    needs: build-with-cache

    permissions:
      id-token: write
      contents: read

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Configure AWS Credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ secrets.AWS_ROLE_ARN }}
          role-session-name: yawl-ci-analysis
          aws-region: us-east-1

      - name: Generate Performance Report
        run: |
          bash scripts/remote-cache-management.sh report --period=7days || true

      - name: Check S3 Storage Usage
        run: |
          BUCKET_NAME="${{ secrets.S3_BUCKET_NAME }}"
          TOTAL_SIZE=$(aws s3api list-objects-v2 \
            --bucket "$BUCKET_NAME" \
            --query 'Contents[].Size' \
            --output text | \
            awk '{sum+=$1} END {print sum}')

          SIZE_GB=$((TOTAL_SIZE / 1024 / 1024 / 1024))

          echo "S3 Cache Size: ${SIZE_GB} GB"

          if [[ $SIZE_GB -gt 5 ]]; then
            echo "âš  WARNING: Cache size exceeds 5GB, consider pruning"
            bash scripts/remote-cache-management.sh prune --days=60 || true
          fi

      - name: Cost Estimation
        run: |
          BUCKET_NAME="${{ secrets.S3_BUCKET_NAME }}"

          # Get storage metrics
          TOTAL_SIZE=$(aws s3api list-objects-v2 \
            --bucket "$BUCKET_NAME" \
            --query 'Contents[].Size' \
            --output text | \
            awk '{sum+=$1} END {print sum}')

          SIZE_GB=$((TOTAL_SIZE / 1024 / 1024 / 1024))

          # Rough AWS S3 pricing (as of 2026)
          STORAGE_COST=$(echo "scale=2; $SIZE_GB * 0.023" | bc)
          TRANSFER_COST=$(echo "scale=2; $SIZE_GB * 0.02" | bc)
          TOTAL_COST=$(echo "scale=2; $STORAGE_COST + $TRANSFER_COST" | bc)

          echo "Monthly S3 Cost Estimate:"
          echo "- Storage (${SIZE_GB}GB @ \$0.023/GB): \$$STORAGE_COST"
          echo "- Transfer (est. \$0.02/GB): \$$TRANSFER_COST"
          echo "- TOTAL: \$$TOTAL_COST"

# Additional jobs can be added:
# - Security scanning (credentials in logs)
# - Performance benchmarking
# - Cost anomaly detection
# - Cache effectiveness analysis
