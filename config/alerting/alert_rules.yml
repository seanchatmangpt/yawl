# =============================================================================
# YAWL Production Alerting Rules
# =============================================================================
groups:
  # Engine Health Alerts
  - name: yawl_engine_alerts
    interval: 30s
    rules:
      - alert: YAWLEngineDown
        expr: up{job="yawl-engine"} == 0
        for: 1m
        labels:
          severity: critical
          service: engine
        annotations:
          summary: "YAWL Engine is down"
          description: "YAWL Engine instance {{ $labels.instance }} has been down for more than 1 minute."

      - alert: YAWLEngineHighMemory
        expr: jvm_memory_used_bytes{area="heap",job="yawl-engine"} / jvm_memory_max_bytes{area="heap",job="yawl-engine"} > 0.9
        for: 5m
        labels:
          severity: warning
          service: engine
        annotations:
          summary: "YAWL Engine high memory usage"
          description: "YAWL Engine heap memory usage is above 90% for 5 minutes."

      - alert: YAWLEngineHighCPU
        expr: system_cpu_usage{job="yawl-engine"} > 0.9
        for: 5m
        labels:
          severity: warning
          service: engine
        annotations:
          summary: "YAWL Engine high CPU usage"
          description: "YAWL Engine CPU usage is above 90% for 5 minutes."

      - alert: YAWLEngineHighGCPause
        expr: rate(jvm_gc_pause_seconds_sum{job="yawl-engine"}[5m]) > 0.5
        for: 5m
        labels:
          severity: warning
          service: engine
        annotations:
          summary: "YAWL Engine high GC pause time"
          description: "YAWL Engine GC pause time is high ({{ $value | humanizeDuration }}/s)."

      - alert: YAWLEngineLowThreadCount
        expr: jvm_threads_live_threads{job="yawl-engine"} < 10
        for: 2m
        labels:
          severity: critical
          service: engine
        annotations:
          summary: "YAWL Engine low thread count"
          description: "YAWL Engine has very few live threads ({{ $value }}), indicating potential deadlock or shutdown."

  # Database Health Alerts
  - name: yawl_database_alerts
    interval: 30s
    rules:
      - alert: PostgreSQLDown
        expr: up{job="postgres"} == 0
        for: 1m
        labels:
          severity: critical
          service: postgres
        annotations:
          summary: "PostgreSQL is down"
          description: "PostgreSQL database has been unreachable for more than 1 minute."

      - alert: PostgreSQLHighConnections
        expr: pg_stat_activity_count / pg_settings_max_connections > 0.8
        for: 5m
        labels:
          severity: warning
          service: postgres
        annotations:
          summary: "PostgreSQL high connection usage"
          description: "PostgreSQL connection usage is above 80% ({{ $value | humanizePercentage }})."

      - alert: PostgreSQLReplicationLag
        expr: pg_replication_lag_seconds > 30
        for: 2m
        labels:
          severity: warning
          service: postgres
        annotations:
          summary: "PostgreSQL replication lag"
          description: "PostgreSQL replication lag is {{ $value }} seconds."

      - alert: PostgreSQLDeadlocks
        expr: rate(pg_stat_database_deadlocks[5m]) > 0.1
        for: 5m
        labels:
          severity: warning
          service: postgres
        annotations:
          summary: "PostgreSQL deadlocks detected"
          description: "PostgreSQL is experiencing deadlocks at {{ $value }}/s."

      - alert: PostgreSQLSlowQueries
        expr: pg_stat_statements_mean_exec_time_seconds > 5
        for: 5m
        labels:
          severity: warning
          service: postgres
        annotations:
          summary: "PostgreSQL slow queries"
          description: "PostgreSQL average query execution time is {{ $value }}s."

  # Service Health Alerts
  - name: yawl_service_alerts
    interval: 30s
    rules:
      - alert: ServiceDown
        expr: up{job=~"yawl-.*"} == 0
        for: 2m
        labels:
          severity: critical
        annotations:
          summary: "YAWL service is down"
          description: "{{ $labels.job }} instance {{ $labels.instance }} has been down for more than 2 minutes."

      - alert: HighErrorRate
        expr: |
          sum(rate(http_server_requests_seconds_count{status=~"5..",job=~"yawl-.*"}[5m])) by (job)
          /
          sum(rate(http_server_requests_seconds_count{job=~"yawl-.*"}[5m])) by (job)
          > 0.05
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: "High error rate detected"
          description: "{{ $labels.job }} is experiencing >5% 5xx errors for 5 minutes."

      - alert: HighLatency
        expr: |
          histogram_quantile(0.95, sum(rate(http_server_requests_seconds_bucket{job=~"yawl-.*"}[5m])) by (le, job)) > 2
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High latency detected"
          description: "{{ $labels.job }} P95 latency is above 2 seconds."

  # Workflow Performance Alerts
  - name: yawl_workflow_alerts
    interval: 60s
    rules:
      - alert: StuckWorkItems
        expr: yawl_work_items_status{status="suspended"} > 100
        for: 10m
        labels:
          severity: warning
          service: engine
        annotations:
          summary: "High number of suspended work items"
          description: "There are {{ $value }} suspended work items, indicating potential workflow issues."

      - alert: WorkflowCaseTimeout
        expr: rate(yawl_case_duration_seconds_count[10m]) == 0 and yawl_active_cases > 0
        for: 15m
        labels:
          severity: warning
          service: engine
        annotations:
          summary: "No workflow cases completing"
          description: "Active cases exist but none are completing. Workflows may be stuck."

  # Infrastructure Alerts
  - name: infrastructure_alerts
    interval: 30s
    rules:
      - alert: DiskSpaceLow
        expr: (node_filesystem_avail_bytes{mountpoint="/"} / node_filesystem_size_bytes{mountpoint="/"}) < 0.15
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: "Disk space low"
          description: "Disk space on {{ $labels.instance }} is below 15% ({{ $value | humanizePercentage }} available)."

      - alert: ContainerRestartingFrequently
        expr: increase(container_restart_count[1h]) > 5
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Container restarting frequently"
          description: "Container {{ $labels.name }} has restarted {{ $value }} times in the last hour."

      - alert: TraefikHighErrorRate
        expr: |
          sum(rate(traefik_service_requests_total{code=~"5.."}[5m])) by (service)
          /
          sum(rate(traefik_service_requests_total[5m])) by (service)
          > 0.1
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: "Traefik high error rate"
          description: "Service {{ $labels.service }} has >10% 5xx errors through Traefik."
