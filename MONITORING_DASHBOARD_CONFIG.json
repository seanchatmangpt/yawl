{
  "apiVersion": "v1",
  "kind": "ConfigMap",
  "metadata": {
    "name": "yawl-monitoring-config",
    "namespace": "yawl-prod"
  },
  "data": {
    "prometheus.yml": "global:\n  scrape_interval: 15s\n  evaluation_interval: 15s\n  external_labels:\n    cluster: 'production'\n    environment: 'prod'\n\nscrape_configs:\n  - job_name: 'yawl-engine'\n    metrics_path: '/actuator/prometheus'\n    static_configs:\n      - targets: ['localhost:9090']\n    scrape_interval: 10s\n    scrape_timeout: 5s\n\n  - job_name: 'postgresql'\n    static_configs:\n      - targets: ['localhost:9187']\n    scrape_interval: 30s\n\n  - job_name: 'node-exporter'\n    static_configs:\n      - targets: ['localhost:9100']\n    scrape_interval: 30s\n\nalerting:\n  alertmanagers:\n    - static_configs:\n        - targets: ['localhost:9093']\n\nrule_files:\n  - '/etc/prometheus/alerts/*.yml'\n",
    "alerts.yml": "groups:\n  - name: yawl_alerts\n    interval: 30s\n    rules:\n      - alert: HighGCPauseTime\n        expr: 'jvm_gc_pause_seconds{quantile=\"0.99\"} > 0.1'\n        for: 5m\n        labels:\n          severity: warning\n        annotations:\n          summary: 'High GC pause time detected ({{ $value }}s)'\n          description: 'GC pause time exceeds 100ms. Pod: {{ $labels.pod }}'\n\n      - alert: CriticalGCPauseTime\n        expr: 'jvm_gc_pause_seconds{quantile=\"0.99\"} > 0.5'\n        for: 1m\n        labels:\n          severity: critical\n        annotations:\n          summary: 'CRITICAL: GC pause time ({{ $value }}s)'\n          runbook_url: 'https://runbooks.internal/gc-pause-time'\n\n      - alert: HighMemoryUsage\n        expr: '(jvm_memory_used_bytes{area=\"heap\"} / jvm_memory_max_bytes{area=\"heap\"}) > 0.85'\n        for: 5m\n        labels:\n          severity: warning\n        annotations:\n          summary: 'High heap memory usage: {{ $value | humanizePercentage }}'\n\n      - alert: OutOfMemoryRisk\n        expr: '(jvm_memory_used_bytes{area=\"heap\"} / jvm_memory_max_bytes{area=\"heap\"}) > 0.95'\n        for: 1m\n        labels:\n          severity: critical\n        annotations:\n          summary: 'CRITICAL: Heap memory near limit'\n          description: 'Heap usage {{ $value | humanizePercentage }}. Restart may be needed.'\n\n      - alert: DatabaseConnectionPoolExhausted\n        expr: 'hikaricp_connections_pending > 0'\n        for: 2m\n        labels:\n          severity: critical\n        annotations:\n          summary: 'Database connection pool exhausted'\n          description: '{{ $value }} connections pending. Query may be stuck.'\n\n      - alert: HighErrorRate\n        expr: 'rate(http_requests_total{status=~\"5..\"}[5m]) > 0.01'\n        for: 5m\n        labels:\n          severity: critical\n        annotations:\n          summary: 'High error rate: {{ $value | humanizePercentage }}'\n\n      - alert: ServiceDown\n        expr: 'up{job=\"yawl-engine\"} == 0'\n        for: 1m\n        labels:\n          severity: critical\n        annotations:\n          summary: 'YAWL service is down'\n          runbook_url: 'https://runbooks.internal/service-down'\n\n      - alert: DatabaseDown\n        expr: 'up{job=\"postgresql\"} == 0'\n        for: 1m\n        labels:\n          severity: critical\n        annotations:\n          summary: 'PostgreSQL database is down'\n\n      - alert: HighLatency\n        expr: 'histogram_quantile(0.99, rate(http_request_duration_seconds_bucket[5m])) > 1.0'\n        for: 10m\n        labels:\n          severity: warning\n        annotations:\n          summary: 'High API latency: {{ $value }}s (p99)'\n\n      - alert: LowThroughput\n        expr: 'rate(http_requests_total[5m]) < 10'\n        for: 15m\n        labels:\n          severity: warning\n        annotations:\n          summary: 'Low throughput: {{ $value }} req/s'\n\n      - alert: DiskSpaceRunningOut\n        expr: '(node_filesystem_avail_bytes / node_filesystem_size_bytes) < 0.1'\n        for: 5m\n        labels:\n          severity: warning\n        annotations:\n          summary: 'Disk space low: {{ $value | humanizePercentage }} remaining'\n\n      - alert: HighCPUUsage\n        expr: '(1 - (rate(node_cpu_seconds_total{mode=\"idle\"}[5m]))) > 0.8'\n        for: 10m\n        labels:\n          severity: warning\n        annotations:\n          summary: 'High CPU usage: {{ $value | humanizePercentage }}'\n",
    "grafana-dashboard.json": "{\n  \"annotations\": {\n    \"list\": [\n      {\n        \"builtIn\": 1,\n        \"datasource\": \"-- Grafana --\",\n        \"enable\": true,\n        \"hide\": true,\n        \"iconColor\": \"rgba(0, 211, 255, 1)\",\n        \"name\": \"Annotations\",\n        \"type\": \"dashboard\"\n      }\n    ]\n  },\n  \"editable\": true,\n  \"gnetId\": null,\n  \"graphTooltip\": 0,\n  \"id\": null,\n  \"links\": [],\n  \"panels\": [\n    {\n      \"datasource\": \"Prometheus\",\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"color\": {\"mode\": \"palette-classic\"},\n          \"custom\": {\n            \"axisLabel\": \"\",\n            \"axisPlacement\": \"auto\",\n            \"barAlignment\": 0,\n            \"drawStyle\": \"line\",\n            \"fillOpacity\": 10,\n            \"gradientMode\": \"none\",\n            \"hideFrom\": {\"tooltip\": false, \"viz\": false, \"legend\": false},\n            \"lineInterpolation\": \"linear\",\n            \"lineWidth\": 1,\n            \"pointSize\": 5,\n            \"scaleDistribution\": {\"type\": \"linear\"},\n            \"showPoints\": \"never\",\n            \"spanNulls\": true,\n            \"stacking\": {\"group\": \"A\", \"mode\": \"none\"},\n            \"thresholdsStyle\": {\"mode\": \"off\"}\n          },\n          \"mappings\": [],\n          \"thresholds\": {\n            \"mode\": \"absolute\",\n            \"steps\": [\n              {\"color\": \"green\", \"value\": null},\n              {\"color\": \"red\", \"value\": 80}\n            ]\n          },\n          \"unit\": \"ms\"\n        },\n        \"overrides\": []\n      },\n      \"gridPos\": {\"h\": 8, \"w\": 12, \"x\": 0, \"y\": 0},\n      \"id\": 2,\n      \"options\": {\n        \"legend\": {\n          \"calcs\": [\"mean\", \"max\", \"last\"],\n          \"displayMode\": \"table\",\n          \"placement\": \"bottom\"\n        },\n        \"tooltip\": {\"mode\": \"single\"}\n      },\n      \"pluginVersion\": \"8.0.0\",\n      \"targets\": [\n        {\n          \"expr\": \"jvm_gc_pause_seconds\",\n          \"refId\": \"A\",\n          \"legendFormat\": \"GC Pause (quantile={{ quantile }})\"\n        }\n      ],\n      \"title\": \"GC Pause Times\",\n      \"type\": \"timeseries\"\n    },\n    {\n      \"datasource\": \"Prometheus\",\n      \"fieldConfig\": {\n        \"defaults\": {\"unit\": \"bytes\"},\n        \"overrides\": []\n      },\n      \"gridPos\": {\"h\": 8, \"w\": 12, \"x\": 12, \"y\": 0},\n      \"id\": 3,\n      \"options\": {\n        \"legend\": {\"displayMode\": \"list\", \"placement\": \"bottom\"},\n        \"tooltip\": {\"mode\": \"single\"}\n      },\n      \"targets\": [\n        {\"expr\": \"jvm_memory_used_bytes{area=\\\"heap\\\"}\", \"legendFormat\": \"Used\"},\n        {\"expr\": \"jvm_memory_max_bytes{area=\\\"heap\\\"}\", \"legendFormat\": \"Max\"}\n      ],\n      \"title\": \"JVM Heap Memory\",\n      \"type\": \"timeseries\"\n    },\n    {\n      \"datasource\": \"Prometheus\",\n      \"fieldConfig\": {\"defaults\": {\"unit\": \"short\"}},\n      \"gridPos\": {\"h\": 8, \"w\": 12, \"x\": 0, \"y\": 8},\n      \"id\": 4,\n      \"options\": {\"legend\": {\"displayMode\": \"list\"}, \"tooltip\": {\"mode\": \"single\"}},\n      \"targets\": [{\"expr\": \"rate(http_requests_total[5m])\", \"legendFormat\": \"Requests/sec\"}],\n      \"title\": \"HTTP Request Rate\",\n      \"type\": \"timeseries\"\n    },\n    {\n      \"datasource\": \"Prometheus\",\n      \"fieldConfig\": {\"defaults\": {\"unit\": \"ms\"}},\n      \"gridPos\": {\"h\": 8, \"w\": 12, \"x\": 12, \"y\": 8},\n      \"id\": 5,\n      \"targets\": [\n        {\"expr\": \"histogram_quantile(0.50, rate(http_request_duration_seconds_bucket[5m])) * 1000\", \"legendFormat\": \"p50\"},\n        {\"expr\": \"histogram_quantile(0.95, rate(http_request_duration_seconds_bucket[5m])) * 1000\", \"legendFormat\": \"p95\"},\n        {\"expr\": \"histogram_quantile(0.99, rate(http_request_duration_seconds_bucket[5m])) * 1000\", \"legendFormat\": \"p99\"}\n      ],\n      \"title\": \"API Latency Percentiles\",\n      \"type\": \"timeseries\"\n    },\n    {\n      \"datasource\": \"Prometheus\",\n      \"fieldConfig\": {\"defaults\": {\"unit\": \"short\"}},\n      \"gridPos\": {\"h\": 8, \"w\": 12, \"x\": 0, \"y\": 16},\n      \"id\": 6,\n      \"targets\": [{\"expr\": \"hikaricp_connections_active\", \"legendFormat\": \"Active Connections\"}],\n      \"title\": \"Database Connection Pool\",\n      \"type\": \"timeseries\"\n    },\n    {\n      \"datasource\": \"Prometheus\",\n      \"fieldConfig\": {\"defaults\": {\"unit\": \"percentunit\"}},\n      \"gridPos\": {\"h\": 8, \"w\": 12, \"x\": 12, \"y\": 16},\n      \"id\": 7,\n      \"targets\": [\n        {\"expr\": \"rate(http_requests_total{status=~\\\"2..\\\"}[5m]) / rate(http_requests_total[5m])\", \"legendFormat\": \"Success Rate\"}\n      ],\n      \"title\": \"Request Success Rate\",\n      \"type\": \"gauge\"\n    }\n  ],\n  \"refresh\": \"30s\",\n  \"schemaVersion\": 27,\n  \"style\": \"dark\",\n  \"tags\": [\"yawl\", \"production\"],\n  \"templating\": {\"list\": []},\n  \"time\": {\"from\": \"now-6h\", \"to\": \"now\"},\n  \"timepicker\": {},\n  \"timezone\": \"UTC\",\n  \"title\": \"YAWL v6.0.0 Production Dashboard\",\n  \"uid\": \"yawl-prod-v1\",\n  \"version\": 1\n}"
  }
}
