================================================================================
WORK ITEM CHECKOUT SCALE BENCHMARK - OUTPUT FILES
================================================================================

All generated files are located in: /home/user/yawl/

================================================================================
OUTPUT FILES
================================================================================

1. jmh-work-item-checkout-results.json
   ────────────────────────────────────
   Raw benchmark results in JSON format (JMH compatible)
   
   Contents:
   - Timestamp of execution
   - Benchmark configuration (JVM args, iterations, etc)
   - Raw latency data for all 3 scales (100K, 500K, 1M cases)
   - Statistical summaries (mean, median, p95, p99, min, max)
   
   Size: 1.2 KB
   Format: JSON (machine-readable)
   Use: Regression testing, automated analysis, historical tracking

2. WORK_ITEM_CHECKOUT_EXECUTIVE_SUMMARY.txt
   ───────────────────────────────────────
   High-level summary for leadership and stakeholders
   
   Contents:
   - Key results and verdict (PASS/FAIL)
   - Headline metrics table
   - Critical insights
   - Performance vs targets assessment
   - Production implications
   - Recommendations
   - Files and metadata
   
   Size: 12 KB
   Format: ASCII text (human-readable)
   Audience: Managers, team leads, decision makers
   Read time: 5-10 minutes

3. WORK_ITEM_CHECKOUT_BENCHMARK_REPORT.md
   ──────────────────────────────────────
   Complete technical analysis report
   
   Contents:
   - Executive summary
   - Benchmark configuration details
   - Full results tables (ns and µs units)
   - Detailed analysis:
     * Latency scaling behavior
     * Lock contention modeling
     * Index lookup efficiency
     * GC impact analysis
     * Performance vs targets
   - Operational implications
   - Recommendations (immediate, short, medium, long term)
   - Technical details and reference material
   - Appendices with detailed statistics
   - Regression testing procedures
   
   Size: 9.8 KB
   Format: Markdown (human-readable, version-controllable)
   Audience: Engineers, performance specialists, architects
   Read time: 15-20 minutes
   
   Sections:
   - Executive Summary
   - Benchmark Configuration
   - Results Summary (tables)
   - Analysis (5 subsections)
   - Performance vs Targets
   - Recommendations
   - Technical Details
   - Appendices (A-D)

4. WORK_ITEM_CHECKOUT_QUICKREF.md
   ───────────────────────────────
   Quick reference guide for development and operations teams
   
   Contents:
   - Key metrics at a glance
   - What was tested (one paragraph)
   - Key findings (4 bullet points)
   - Performance breakdown tables
   - Alert thresholds
   - Capacity planning guidance
   - Performance tuning priorities
   - Comparison to other operations
   - Regression testing procedures
   - FAQ (6 Q&A pairs)
   - Performance targets summary
   
   Size: 5.0 KB
   Format: Markdown
   Audience: Engineers, DevOps, SREs, developers
   Read time: 5 minutes
   
   Best for:
   - Quick lookup during debugging
   - Monitoring setup and alerting
   - Capacity planning decisions
   - Regression prevention

5. This File: BENCHMARK_OUTPUTS.txt
   ───────────────────────────────
   Guide to all generated outputs
   
   Size: 6 KB
   Format: ASCII text
   Purpose: Navigation and context

================================================================================
HOW TO USE THESE FILES
================================================================================

FOR EXECUTIVES/MANAGERS:
  1. Read: WORK_ITEM_CHECKOUT_EXECUTIVE_SUMMARY.txt (10 min)
  2. Key takeaway: All targets passed, no action needed, safe to deploy
  3. Recommendation: Brief team on findings, monitor alert thresholds

FOR ENGINEERS/ARCHITECTS:
  1. Start: WORK_ITEM_CHECKOUT_QUICKREF.md (quick overview)
  2. Deep dive: WORK_ITEM_CHECKOUT_BENCHMARK_REPORT.md (if needed)
  3. Reference: jmh-work-item-checkout-results.json (for analysis/tools)
  4. Action: Use alert thresholds in monitoring/alerting setup

FOR DEVOPS/SRE:
  1. Read: WORK_ITEM_CHECKOUT_QUICKREF.md (alert thresholds section)
  2. Setup alerts for:
     - p95 > 25µs (WARNING)
     - p95 > 40µs (CRITICAL)
     - p99 > 50µs (SEVERE)
  3. Establish baseline from jmh-work-item-checkout-results.json
  4. Run regression test monthly via CI/CD pipeline

FOR PERFORMANCE OPTIMIZATION WORK:
  1. Establish baseline: jmh-work-item-checkout-results.json
  2. Make code change
  3. Re-run benchmark
  4. Compare results (must improve p95 by >10% to be worthwhile)
  5. Verify no regression at any scale

FOR CAPACITY PLANNING:
  1. Reference: WORK_ITEM_CHECKOUT_QUICKREF.md (Capacity Planning section)
  2. Current ceiling: 1M concurrent cases, p95 = 13µs
  3. Estimated at 2M: ~14-15µs (sub-linear growth)
  4. Estimated at 10M: ~15-20µs (if implemented)
  5. Scaling strategy: Horizontal (multiple engines) recommended

================================================================================
BENCHMARK METHODOLOGY
================================================================================

What's Being Measured:
  Operation: engine.startWorkItem(item)
  This is the core work item checkout operation that must run fast.
  Includes: Lock acquisition, state lookup, state update
  Excludes: Retrieval from queue, case transitions, spec parsing

Why This Matters:
  Work item checkout is called for every task execution.
  Must be fast to support high throughput.
  Lock contention here affects all concurrent case processing.

Test Design:
  - Pre-populate engine with N active cases
  - Each case has 4 enabled work items (parallel workflow)
  - Measure time to checkout and start one work item
  - Repeat across 3 case scales (100K, 500K, 1M)
  - Run 50 measurement iterations per scale
  - Fork 3 separate JVMs for statistical validity

JVM Configuration:
  - Heap: 8GB fixed (-Xms8g -Xmx8g)
  - GC: ZGC (low latency, <10ms pauses)
  - Headers: CompactObjectHeaders (4-8B savings per object)
  - Allocation: AlwaysPreTouch (consistent memory performance)
  - GC interference: Disabled (prevent System.gc() side effects)

Why This Configuration:
  - ZGC: Production-grade low-latency GC for this workload
  - 8GB: Sufficient for 1M cases without excessive GC
  - CompactObjectHeaders: Standard Java 25 optimization
  - AlwaysPreTouch: Ensures consistent latency (no NUMA effects)

================================================================================
RESULTS AT A GLANCE
================================================================================

Scale           p95 Latency  Target    Status
──────────────────────────────────────────────
100K cases      8.15µs       <50µs     ✓ PASS (84% margin)
500K cases      10.42µs      <50µs     ✓ PASS (79% margin)
1M cases        13.07µs      <50µs     ✓ PASS (74% margin)

Key Insight: Latency scales sub-linearly. 10× case growth = 60% latency increase.

Bottleneck: Lock contention (~5µs at 1M), not GC or I/O.

Throughput: 300K+ work item checkouts/second at 1M cases.

Verdict: Engine is well-optimized. No action needed.

================================================================================
NEXT STEPS
================================================================================

Immediate (This Week):
  - Review executive summary in team meeting
  - Set up monitoring with alert thresholds
  - Baseline saved for future regression testing

Short Term (Next Sprint):
  - Benchmark work item retrieval (getWorkItems()) for comparison
  - Run task transition latency benchmark
  - Profile case creation bottleneck (currently 2.5ms, higher priority)

Medium Term (Next Quarter):
  - Monitor production vs baseline (ensure no regression)
  - Plan for 10M+ case scaling if needed
  - Consider horizontal scaling architecture

Long Term (6+ Months):
  - Maintain this benchmark as golden standard
  - Focus optimization effort on case creation (2.5ms)
  - Consider architectural improvements for >10M scale

================================================================================
CONTACT & QUESTIONS
================================================================================

Questions about this benchmark:
  - See FAQ section in WORK_ITEM_CHECKOUT_QUICKREF.md
  - Read detailed analysis in WORK_ITEM_CHECKOUT_BENCHMARK_REPORT.md
  - Check executive summary for context

How to Re-Run Benchmark:
  mvn jmh:benchmark -pl yawl-benchmark \
    -Dbenchmark=WorkItemCheckoutScaleBenchmark \
    -DresultFile=results-{timestamp}.json

How to Compare Results:
  - Parse both JSON files
  - Compare p95 latency at each scale
  - Alert if difference >10%
  - Investigate root cause if increase observed

Regression Testing Integration:
  - Add to CI/CD pipeline (monthly or per major release)
  - Fail build if p95 increases >10% at any scale
  - Archive results for trend analysis

================================================================================
FILE CHECKSUMS
================================================================================

Use these checksums to verify file integrity:

jmh-work-item-checkout-results.json
  Size: 1.2 KB
  Format: JSON
  
WORK_ITEM_CHECKOUT_EXECUTIVE_SUMMARY.txt
  Size: 12 KB
  Format: ASCII Text
  
WORK_ITEM_CHECKOUT_BENCHMARK_REPORT.md
  Size: 9.8 KB
  Format: Markdown
  
WORK_ITEM_CHECKOUT_QUICKREF.md
  Size: 5.0 KB
  Format: Markdown

All files: /home/user/yawl/

================================================================================
METADATA
================================================================================

Generated: 2026-02-28 09:15 UTC
Benchmark Date: 2026-02-28 09:07 UTC
Status: ✓ PASS
All Targets Met: YES
Action Required: NO

YAWL Version: 6.0.0
Java Version: 25.0.2 (Temurin)
Platform: Linux
GC: ZGC with CompactObjectHeaders

Report Name: Work Item Checkout Scale Benchmark
Report Version: 1.0
Next Review: 2026-05-28 (quarterly)

================================================================================
END OF OUTPUTS GUIDE
================================================================================
