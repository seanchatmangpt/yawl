# =============================================================================
# YAWL v6.0.0 â€“ Helm Chart Default Values
# =============================================================================
# Override these values with environment-specific values files:
#   helm install yawl ./helm/yawl -f values-aws.yaml
#   helm install yawl ./helm/yawl -f values-gcp.yaml
# =============================================================================

# -----------------------------------------------------------------------------
# Global Settings
# -----------------------------------------------------------------------------
global:
  imageRegistry: ""
  imagePullSecrets: []
  storageClass: ""
  namespace: yawl
  labels: {}
  annotations: {}

# -----------------------------------------------------------------------------
# YAWL Version
# -----------------------------------------------------------------------------
yawlVersion: "6.0.0"
buildNumber: "1"

# -----------------------------------------------------------------------------
# Cloud Provider Configuration
# One of: gcp, aws, azure, oracle, ibm, or "" for generic
# -----------------------------------------------------------------------------
cloudProvider: ""

# -----------------------------------------------------------------------------
# Image Configuration
# -----------------------------------------------------------------------------
image:
  registry: ghcr.io
  repository: yawlfoundation/yawl/engine
  tag: "6.0.0"
  pullPolicy: IfNotPresent
  # Image digest for immutable reference (set in production)
  digest: ""

# -----------------------------------------------------------------------------
# Database Configuration
# -----------------------------------------------------------------------------
postgresql:
  enabled: true
  auth:
    postgresPassword: ""    # Provide via --set or external secret
    password: ""
    database: "yawl"
    existingSecret: ""
  primary:
    persistence:
      enabled: true
      size: 20Gi
      storageClass: ""
    resources:
      requests:
        cpu: 250m
        memory: 512Mi
      limits:
        cpu: 1000m
        memory: 2Gi
  readReplicas:
    replicaCount: 0
  metrics:
    enabled: true
  backup:
    enabled: true
    schedule: "0 2 * * *"
    retentionDays: 7

# External PostgreSQL (when postgresql.enabled is false)
externalDatabase:
  host: ""
  port: 5432
  database: yawl
  user: postgres
  password: ""
  existingSecret: ""
  existingSecretPasswordKey: password

# -----------------------------------------------------------------------------
# Redis Configuration
# -----------------------------------------------------------------------------
redis:
  enabled: true
  auth:
    enabled: true
    password: ""
  master:
    persistence:
      enabled: true
      size: 4Gi
  replica:
    replicaCount: 1

externalRedis:
  host: ""
  port: 6379
  password: ""
  existingSecret: ""
  existingSecretPasswordKey: password

# -----------------------------------------------------------------------------
# Service Definitions (7 Core Services)
# -----------------------------------------------------------------------------
services:
  engine:
    name: engine
    enabled: true
    replicaCount: 2
    image:
      repository: yawlfoundation/yawl/engine
      tag: ""
      pullPolicy: IfNotPresent
    # Set clustering.enabled=true to use StatefulSet instead of Deployment
    clustering:
      enabled: false
    persistence:
      size: 5Gi
      storageClass: ""
    service:
      type: ClusterIP
      port: 8080
      targetPort: 8080
      managementPort: 9090
      annotations: {}
    resources:
      requests:
        cpu: 500m
        memory: 1Gi
      limits:
        cpu: 2000m
        memory: 2Gi
    autoscaling:
      enabled: true
      minReplicas: 2
      maxReplicas: 10
      targetCPUUtilizationPercentage: 70
      targetMemoryUtilizationPercentage: 80
    # Probes use Spring Boot Actuator on management port 9090
    livenessProbe:
      httpGet:
        path: /actuator/health/liveness
        port: 9090
      initialDelaySeconds: 0
      periodSeconds: 15
      timeoutSeconds: 5
      failureThreshold: 3
    readinessProbe:
      httpGet:
        path: /actuator/health/readiness
        port: 9090
      initialDelaySeconds: 0
      periodSeconds: 10
      timeoutSeconds: 3
      failureThreshold: 3
    startupProbe:
      httpGet:
        path: /actuator/health/liveness
        port: 9090
      initialDelaySeconds: 30
      periodSeconds: 10
      timeoutSeconds: 5
      failureThreshold: 18
    annotations: {}
    podLabels: {}
    podAnnotations: {}
    env: []
    extraVolumes: []
    extraVolumeMounts: []

  resourceService:
    name: resource-service
    enabled: true
    replicaCount: 2
    image:
      repository: yawlfoundation/yawl/resource-service
      tag: ""
      pullPolicy: IfNotPresent
    service:
      type: ClusterIP
      port: 8080
      targetPort: 8080
      managementPort: 9090
    resources:
      requests:
        cpu: 250m
        memory: 512Mi
      limits:
        cpu: 1000m
        memory: 1Gi
    autoscaling:
      enabled: true
      minReplicas: 2
      maxReplicas: 8
      targetCPUUtilizationPercentage: 70
      targetMemoryUtilizationPercentage: 80
    livenessProbe:
      httpGet:
        path: /actuator/health/liveness
        port: 9090
      initialDelaySeconds: 0
      periodSeconds: 15
      timeoutSeconds: 5
      failureThreshold: 3
    readinessProbe:
      httpGet:
        path: /actuator/health/readiness
        port: 9090
      initialDelaySeconds: 0
      periodSeconds: 10
      timeoutSeconds: 3
      failureThreshold: 3
    startupProbe:
      httpGet:
        path: /actuator/health/liveness
        port: 9090
      initialDelaySeconds: 20
      periodSeconds: 10
      failureThreshold: 12
    env: []
    extraVolumes: []
    extraVolumeMounts: []

  workletService:
    name: worklet-service
    enabled: true
    replicaCount: 2
    image:
      repository: yawlfoundation/yawl/worklet-service
      tag: ""
      pullPolicy: IfNotPresent
    service:
      type: ClusterIP
      port: 8080
      targetPort: 8080
      managementPort: 9090
    resources:
      requests:
        cpu: 250m
        memory: 512Mi
      limits:
        cpu: 1000m
        memory: 1Gi
    autoscaling:
      enabled: true
      minReplicas: 2
      maxReplicas: 6
      targetCPUUtilizationPercentage: 70
      targetMemoryUtilizationPercentage: 80
    livenessProbe:
      httpGet:
        path: /actuator/health/liveness
        port: 9090
      initialDelaySeconds: 0
      periodSeconds: 15
      timeoutSeconds: 5
      failureThreshold: 3
    readinessProbe:
      httpGet:
        path: /actuator/health/readiness
        port: 9090
      initialDelaySeconds: 0
      periodSeconds: 10
      timeoutSeconds: 3
      failureThreshold: 3
    startupProbe:
      httpGet:
        path: /actuator/health/liveness
        port: 9090
      initialDelaySeconds: 20
      periodSeconds: 10
      failureThreshold: 12
    env: []
    extraVolumes: []
    extraVolumeMounts: []

  monitorService:
    name: monitor-service
    enabled: true
    replicaCount: 1
    image:
      repository: yawlfoundation/yawl/monitor-service
      tag: ""
    service:
      type: ClusterIP
      port: 8080
      targetPort: 8080
      managementPort: 9090
    resources:
      requests:
        cpu: 100m
        memory: 256Mi
      limits:
        cpu: 500m
        memory: 512Mi
    autoscaling:
      enabled: false
    livenessProbe:
      httpGet:
        path: /actuator/health/liveness
        port: 9090
      initialDelaySeconds: 0
      periodSeconds: 15
      failureThreshold: 3
    readinessProbe:
      httpGet:
        path: /actuator/health/readiness
        port: 9090
      initialDelaySeconds: 0
      periodSeconds: 10
      failureThreshold: 3
    env: []
    extraVolumes: []
    extraVolumeMounts: []

  costService:
    name: cost-service
    enabled: true
    replicaCount: 1
    image:
      repository: yawlfoundation/yawl/cost-service
      tag: ""
    service:
      type: ClusterIP
      port: 8080
      targetPort: 8080
      managementPort: 9090
    resources:
      requests:
        cpu: 100m
        memory: 256Mi
      limits:
        cpu: 500m
        memory: 512Mi
    autoscaling:
      enabled: false
    livenessProbe:
      httpGet:
        path: /actuator/health/liveness
        port: 9090
      initialDelaySeconds: 0
      periodSeconds: 15
      failureThreshold: 3
    readinessProbe:
      httpGet:
        path: /actuator/health/readiness
        port: 9090
      initialDelaySeconds: 0
      periodSeconds: 10
      failureThreshold: 3
    env: []
    extraVolumes: []
    extraVolumeMounts: []

  schedulingService:
    name: scheduling-service
    enabled: true
    replicaCount: 1
    image:
      repository: yawlfoundation/yawl/scheduling-service
      tag: ""
    service:
      type: ClusterIP
      port: 8080
      targetPort: 8080
      managementPort: 9090
    resources:
      requests:
        cpu: 100m
        memory: 256Mi
      limits:
        cpu: 500m
        memory: 512Mi
    autoscaling:
      enabled: false
    livenessProbe:
      httpGet:
        path: /actuator/health/liveness
        port: 9090
      initialDelaySeconds: 0
      periodSeconds: 15
      failureThreshold: 3
    readinessProbe:
      httpGet:
        path: /actuator/health/readiness
        port: 9090
      initialDelaySeconds: 0
      periodSeconds: 10
      failureThreshold: 3
    env: []
    extraVolumes: []
    extraVolumeMounts: []

  balancer:
    name: balancer
    enabled: true
    replicaCount: 2
    image:
      repository: yawlfoundation/yawl/balancer
      tag: ""
    service:
      type: ClusterIP
      port: 8080
      targetPort: 8080
      managementPort: 9090
    resources:
      requests:
        cpu: 100m
        memory: 256Mi
      limits:
        cpu: 500m
        memory: 512Mi
    autoscaling:
      enabled: true
      minReplicas: 2
      maxReplicas: 5
    livenessProbe:
      httpGet:
        path: /actuator/health/liveness
        port: 9090
      initialDelaySeconds: 0
      periodSeconds: 15
      failureThreshold: 3
    readinessProbe:
      httpGet:
        path: /actuator/health/readiness
        port: 9090
      initialDelaySeconds: 0
      periodSeconds: 10
      failureThreshold: 3
    env: []
    extraVolumes: []
    extraVolumeMounts: []

# -----------------------------------------------------------------------------
# Ingress
# -----------------------------------------------------------------------------
ingress:
  enabled: true
  className: nginx
  annotations:
    nginx.ingress.kubernetes.io/ssl-redirect: "true"
    nginx.ingress.kubernetes.io/proxy-body-size: "50m"
    nginx.ingress.kubernetes.io/proxy-connect-timeout: "60"
    nginx.ingress.kubernetes.io/proxy-send-timeout: "300"
    nginx.ingress.kubernetes.io/proxy-read-timeout: "300"
    nginx.ingress.kubernetes.io/configuration-snippet: |
      add_header X-Frame-Options "SAMEORIGIN" always;
      add_header X-Content-Type-Options "nosniff" always;
      add_header X-XSS-Protection "1; mode=block" always;
      add_header Referrer-Policy "strict-origin-when-cross-origin" always;
      add_header Strict-Transport-Security "max-age=31536000; includeSubDomains" always;
    cert-manager.io/cluster-issuer: letsencrypt-prod
  hosts:
    - host: yawl.example.com
      paths:
        - path: /yawl
          pathType: Prefix
          service: engine
        - path: /resourceService
          pathType: Prefix
          service: resource-service
        - path: /workletService
          pathType: Prefix
          service: worklet-service
        - path: /monitorService
          pathType: Prefix
          service: monitor-service
        - path: /costService
          pathType: Prefix
          service: cost-service
        - path: /schedulingService
          pathType: Prefix
          service: scheduling-service
        - path: /balancer
          pathType: Prefix
          service: balancer
  tls:
    - secretName: yawl-tls-secret
      hosts:
        - yawl.example.com

# -----------------------------------------------------------------------------
# Service Account
# -----------------------------------------------------------------------------
serviceAccount:
  create: true
  name: ""
  annotations: {}
  labels: {}
  automountServiceAccountToken: false   # v6: disabled by default, override per-container

# -----------------------------------------------------------------------------
# Pod Disruption Budget
# -----------------------------------------------------------------------------
podDisruptionBudget:
  enabled: true
  minAvailable: 1

# -----------------------------------------------------------------------------
# Network Policy (default-deny with explicit allow-list)
# -----------------------------------------------------------------------------
networkPolicy:
  enabled: true
  ingress:
    - from:
        - namespaceSelector:
            matchLabels:
              kubernetes.io/metadata.name: ingress-nginx
        - namespaceSelector:
            matchLabels:
              kubernetes.io/metadata.name: monitoring
      ports:
        - protocol: TCP
          port: 8080
        - protocol: TCP
          port: 9090
  egress:
    - to:
        - namespaceSelector: {}
      ports:
        - protocol: TCP
          port: 5432    # PostgreSQL
        - protocol: TCP
          port: 6379    # Redis
        - protocol: TCP
          port: 4317    # OTLP gRPC
        - protocol: TCP
          port: 4318    # OTLP HTTP
    - to: []            # DNS
      ports:
        - protocol: TCP
          port: 53
        - protocol: UDP
          port: 53

# -----------------------------------------------------------------------------
# Pod Security Context (enforced by Kyverno policy; UID must be >= 10001)
# -----------------------------------------------------------------------------
podSecurityContext:
  runAsNonRoot: true
  runAsUser: 10001
  runAsGroup: 10001
  fsGroup: 10001
  fsGroupChangePolicy: OnRootMismatch
  seccompProfile:
    type: RuntimeDefault

containerSecurityContext:
  allowPrivilegeEscalation: false
  capabilities:
    drop: [ALL]
  readOnlyRootFilesystem: true
  runAsNonRoot: true
  runAsUser: 10001
  runAsGroup: 10001

# -----------------------------------------------------------------------------
# JVM Configuration
# -----------------------------------------------------------------------------
jvm:
  opts: >-
    -XX:+UseContainerSupport
    -XX:MaxRAMPercentage=75.0
    -XX:InitialRAMPercentage=25.0
    -XX:+UseZGC
    -XX:+ZGenerational
    -XX:+UseStringDeduplication
    -XX:+ExitOnOutOfMemoryError
    -Djdk.virtualThreadScheduler.parallelism=64
    -Djdk.virtualThreadScheduler.maxPoolSize=512
    -Djava.security.egd=file:/dev/./urandom

# -----------------------------------------------------------------------------
# Logging
# -----------------------------------------------------------------------------
logging:
  yawl: INFO
  worklet: INFO
  resource: INFO
  scheduling: INFO
  proclet: INFO
  hibernate: ERROR
  root: WARN
  mail: INFO

# -----------------------------------------------------------------------------
# Secrets Configuration
# -----------------------------------------------------------------------------
secrets:
  database:
    create: true
    name: ""
    user: postgres
    password: ""          # Override via --set secrets.database.password=<value>
  apiKeys:
    create: true
    name: ""
    engineApiKey: ""
    resourceServiceKey: ""
    workletServiceKey: ""

# -----------------------------------------------------------------------------
# Istio Service Mesh (disabled by default)
# -----------------------------------------------------------------------------
istio:
  enabled: false
  ingressNamespace: istio-system
  canary:
    enabled: false
    weight: 0             # percentage of traffic to canary subset
    stableWeight: 100

# -----------------------------------------------------------------------------
# Monitoring (Prometheus Operator)
# -----------------------------------------------------------------------------
monitoring:
  enabled: true
  namespace: ""           # defaults to release namespace
  scrapeInterval: 30s
  scrapeTimeout: 10s
  serviceMonitorLabels:
    release: prometheus   # match your Prometheus Operator release name
  ruleLabels:
    release: prometheus

# -----------------------------------------------------------------------------
# Cloud-Specific Proxies
# -----------------------------------------------------------------------------
cloudsqlProxy:
  enabled: false
  instanceConnectionName: ""
  serviceAccount: ""
  serviceAccountJson: ""
  proxyPort: 5432

awsRdsProxy:
  enabled: false
  endpoint: ""
  port: 5432

azureDatabase:
  enabled: false
  serverName: ""

oracleDatabase:
  enabled: false
  ocid: ""

ibmDatabase:
  enabled: false
  instanceId: ""

# -----------------------------------------------------------------------------
# Scheduling
# -----------------------------------------------------------------------------
nodeSelector: {}
tolerations: []
affinity: {}
topologySpreadConstraints: []
priorityClassName: ""
terminationGracePeriodSeconds: 60

# -----------------------------------------------------------------------------
# Init Containers / Extra Resources
# -----------------------------------------------------------------------------
initContainers: []
extraDeploy: []
